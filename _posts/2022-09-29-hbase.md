---
title: HBase
categories: [Computer engineering, Cloud infrastructure]
tags: [infrastructure, database, HBase, 인프라, 데이터베이스]
---

네이버클라우드에서 인턴십을 하며 공부한 내용을 정리하는 세 번째 포스팅 시간입니다. 이번 포스팅에서는 HBase라는 데이터베이스에 대해 알아보도록 하겠습니다.

## HBase 개요

HBase는 Hadoop의 HDFS위에 만들어진 분산 컬럼 기반의 데이터베이스입니다. 구조화된 대용량의 데이터에 빠른 임의 접근을 제공하는 구글의 BigTable에 기초하여 HDFS의 데이터에 대한 실시간 임의 읽기/쓰기를 제공합니다. 

HBase는 아래와 같은 특징을 가집니다.

1. Scalability(확장성)

2. Random access with low latency(빠른 임의 접근)

3. High write throughput(높은 쓰기 처리량)

본격적으로 HBase의 구조에 대해 알아보기 전에 HBase와 관련된 LSM-Tree 자료 구조를 알아보겠습니다. 추가적으로 LSM-Tree의 디스크 컴포넌트 구현 방식인 SSTable 먼저 알아보겠습니다. 참고로 SSTable은 HBase의 전신이 되는 Google BigTable 논문에서 먼저 소개되었습니다. 

## SSTable(Sorted Strings Table)

![sstable.png](/assets/img/navercloud/sstable.png){: w="100%" h="100%" style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;"}

SSTable은 Key-Value 쌍으로 구성된 파일이며 Key, Value는 모두 바이트 배열입니다. 한번 만들어진 SSTable은 불변(immutable)하며, SSTable의 Write 작업은 임의의 순서로 일어나므로 SSTable을 만들기 위해서는 Red-Black Tree, AVL Tree같은 Balanced Tree를 활용해 정렬된 채로 메모리에 버퍼해 둡니다. 이 메모리 공간은 memtable이라고 부릅니다. 그 후 일정 크기 이상으로 메모리에 버퍼 데이터가 쌓이면 이걸 디스크에 SSTable 파일로 flush 하여 Key에 따른 정렬을 유지하며 저장하게 됩니다.

* 이렇게 따로 디스크에 SSTable에 flush 된다면, 이들끼리는 정렬된 상태가 아닐텐데 어떻게 처리할까?

    - 그건 merge sort 알고리즘을 사용하여 SSTable들을 병합합니다. SSTable로 저장된 여러개의 segment (디스크 파일)이 있다고 생각해보면, segment 내 데이터들은 key를 기준으로 이미 정렬되어 있으므로 각 segment 별 가장 앞의 데이터만 비교하여 우선순위가 높은 값을 가져와 새로운 segment에  넣어줍니다. 이렇게 하면 O(n)의 빠른 속도로 정렬된 상태를 유지하며 병합할 수 있습니다. 

## LSM Tree(Log-Structured Merge Tree)

![lsm-tree.png](/assets/img/navercloud/lsm-tree.png){: w="100%" h="100%" style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;"}

Log-Structured는 데이터를 순차적으로 디스크에 기록하기 때문에 Log-Structured file system에서 따온 이름입니다. LSM은 Append-only storage로, 파일을 수정할 수 없기 때문에 Merge sort 방식으로 파일을 합치면서 각 Key에 대해 최신 데이터만 유지합니다. 이렇게 하면서 읽기 작업에 필요한 파일의 수를 줄이고 공간도 확보할 수 있습니다.

LSM Tree는 작은 공간의 메모리 컴포넌트와 큰 공간의 디스크 컴포넌트로 구성됩니다. 

1. 메모리 컴포넌트 (mutable): 인 메모리 테이블 혹은 memtable이라고도 부릅니다. LSM Tree는 데이터를 디스크에 바로 쓰는 것이 아니라 memtable에 변경 사항을 버퍼합니다. 읽기, 쓰기 작업 모두 memtable을 거쳐가게 되고 지정값 이상으로 메모리가 차면 디스크에 영구적으로 기록합니다. 무엇보다도, 디스크 접근 없이 메모리에 데이터를 쓰기 때문에 LSM 구조의 저장소는 쓰기 작업 성능 및 처리량이 좋은 편입니다. 또한 데이터의 지속성(Durability)를 보장하기 위해 memtable과 별개로 WAL(Write-Ahead Log)를 둡니다. 

2. 디스크 컴포넌트 (immutable): 메모리에 버퍼된 내용을 영구적으로 담을 때 쓰입니다. 디스크에 기록된 내용은 변경할 수 없기 때문에 오로지 읽기 작업에만 사용됩니다. 읽기 요청이 들어오면 memtable에서 먼저 검색하고 없으면 디스크에서 가장 최신의 SSTable segment부터 검색합니다.

## HBase Architecture

![architecture.jpg](/assets/img/navercloud/architecture.jpg){: w="100%" h="100%" style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;"}

## HBase Master

HBase Master는 HBase 클러스터를 전반적으로 관리하고 어드민 작업 요청을 처리합니다. 

## HBase RegionServer

크기가 큰 테이블을 하나의 서버에만 저장하게 하면 요청을 분산시킬 수 없습니다. 그래서 테이블을 여러 "Region"에 분할하여 할당합니다. Region을 serving하는 호스트를 RegionServer라고 부르며 하나의 RegionServer는 여러 Region을 serving할 수 있습니다. 만약 1개의 RegionServer가 죽으면, ZooKeeper가 이를 탐지하고 Master에게 알려 Master가 다른 서버에 죽은 RegionServer가 Serving하던 Region을 할당합니다. 새로운 RegionServer는 WAL으로 죽은 RegionServer에 있던 Memstore 데이터를 복구합니다.

Region을 더 나누고 옮기는 작업 등을 통해서 RegionServer의 워크로드를 밸런싱 시킬 수 있습니다. Region split에 관여하는 정책을 지정하여 분할할 수도 있고 사용자가 수동으로 split 시킬 수도 있습니다. 이런 방식으로 RegionServer만 더 투입하면 읽기,쓰기 요청 처리량을 향상시킬 수 있습니다.

## ZooKeeper

HBase는 ZooKeeper에 클러스터의 설정 정보, 실행 중인 노드, 테이블 메타 정보 등을 저장합니다. ZooKeeper를 사용하면 클러스터 내 설정 정보를 동기화된 상태로 유지할 수 있으며 클라이언트에서 데이터에 접근할 때 찾아가야 하는 RegionServer 호스트 정보를 제공할 수 있습니다. 마스터는 RegionServer의 status를 감시하면서 사용할 수 있는 노드 정보를 꾸준히 ZooKeeper에 업데이트 합니다.

이러한 이유로 HBase 클러스터를 생성하려면 ZooKeeper가 필요하며 "hbase-site.xml"에서 아래와 같은 설정을 추가해야 합니다.

* "hbase.zookeeper.quorum": HBase에 연결할 ZooKeeper 쿼럼 주소

* "zookeeper.znode.parent": HBase 데이터가 저장될 znode의 경로. 이 경로 아래로 znode가 생성됩니다. 

추가적으로, HBase shell에서 "zk_dump"를 실행하면 ZooKeeper에서 현재 HBase에 관련된 모든 정보를 살펴볼 수 있습니다.

## HBase 읽기/쓰기 과정

먼저 HBase에 데이터를 읽거나 쓸 때 대상이 되는 RegionServer를 찾아야 합니다.

1. 클라이언트는 ZooKeeper에 요청을 보내서 META table을 호스트 하는 RegionServer를 알아냅니다. 해당 RegionServer는 "\${zookeeper.znode.parent}/meta-region-server"의 znode에서 확인 할 수 있습니다.

2. 클라이언트는 META table을 확인해서, 어느 RegionServer로 부터 데이터를 읽고/써야 할지를 알아냅니다. 

## 쓰기

memstore는 HBase가 디스크에 데이터를 쓰기 전에 저장해두고 있는 쓰기 버퍼입니다. 이 버퍼는 공간(hbase.hregion.memstore.flush.size)이 다 차면 HFile 형식으로 디스크에 저장(flush)합니다. HFile은 한 번 쓰면 변하지 않으며 flush가 일어날 때 마다 새로운 HFile이 생성됩니다. HFile은 하나의 컬럼 패밀리에 종속되어 있으며, 한 개의 컬럼 패밀리는 여러 개의 HFile을 가질 수 있습니다. 컬럼 패밀리마다 memstore는 1개씩만 존재합니다.

데이터를 기록할 RegionServer에서 먼저 WAL(Write-Ahead-Log)에 변경 사항을 기록한 뒤 memstore에 기록합니다. 쓰기 작업은 WAL과 memstore 모두 기록을 성공했을 때 끝난 것으로 봅니다.

## 읽기

HBase에서 데이터 읽기 과정은 Block Cache, memstore, HFile을 거치게 됩니다. BlockCache는 HBase가 HFile에서 자주 읽는 데이터를 메모리에 두려는 목적으로 사용하는 캐시입니다. BlockCache와 memstore는 JVM 힙에 존재하며 컬럼 패밀리 하나 당 1개의 BlockCache를 가지고 있습니다. 

1. 읽으려고 하는 데이터가 있는 RegionServer에서 먼저 memstore를 확인합니다. 있으면 클라이언트에게 전달합니다. 이렇게 하는 이유는 읽으려고 하는 데이터의 최근 변경 사항을 확인하기 위해서 입니다.

2. memstore에 데이터가 없으면 그 다음으로 BlockCache를 확인합니다. 있으면 클라이언트에게 전달합니다.

3. 그래도 데이터를 못찾았으면 HFile에서 데이터를 찾습니다. 클라이언트에게 전달하고 BlockCache에 이 데이터를 씁니다.

## 논리 데이터 모델

![hbase-logical-table.png](/assets/img/navercloud/hbase-logical-table.png){: w="100%" h="100%" style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;"}

HBase의 데이터는 테이블에 저장되며 위 예시에서 info가 컬럼 패밀리입니다. 각 로우의 컬럼 값은 cell이라고 부릅니다. 각 로우는 로우를 identify 할 수 있는 rowkey를 가지고 있습니다. 이 키는 모두 바이트 배열입니다. memtable에서 이미 정렬된 상태로 내려오기 때문에 rowkey에 의해 사전식 순서대로 정렬되어 있습니다.

하나의 로우는 임의 갯수의 컬럼을 가질 수 있습니다. 위 테이블에서 shawn로우는 postcode 컬럼을 가지고 있지 않은 것이 예시 입니다. 이렇게 부분 부분 cell에 값이 없을 수 있는데 이런 것을 sparse하다고 표현합니다.

로우 단위로 이루어지는 읽기, 쓰기 요청에 대한 ACID가 보장됩니다.

컬럼 패밀리는 물리적으로 비슷한 컬럼들의 집합입니다. 어떤 컬럼을 종종 같이 접근한다면, 이런 컬럼들은 같은 컬럼 패밀리로 묶습니다. memtable, HFile이 컬럼 패밀리마다 각각 존재하기 때문에 관련이 없는 셀과는 HFile을 분리할 수 있게 됩니다. 또한, 동시에 서로 다른 컬럼 패밀리의 컬럼을 접근하는 것은 하나의 컬럼 패밀리만 접근할 때보다 성능이 떨어집니다.

## 물리 데이터 모델

![hbase-physical-table.png](/assets/img/navercloud/hbase-physical-table.png){: w="100%" h="100%" style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;"}

위에서는 논리적 모델을 살펴봤다면 지금부터는 실제로 HFile에서는 어떻게 저장되어 있는지 알아보겠습니다. rowkey, column, timestamp가 cell에 대한 identifier가 됩니다. 로우는 이 세 값을 통해 indexing되고 이 세 값만 있으면 한번에 cell을 읽을 수 있습니다.

## 구조로 알아보는 HBase의 특징

1. Scalability(확장성): HBase의 읽기, 쓰기 요청은 Slave Node 역할을 하는 RegionServer에서 처리합니다. 많은 요청량을 처리해야 한다면 RegionServer를 추가해서 노드 당 워크로드를 분산시켜줄 수 있습니다. 이때 보통 데이터 Locality를 위해서 HDFS DataNode와 같은 노드에 위치시키므로, DataNode도 같이 추가한다고 보면 됩니다.

2. Random access with low latency(빠른 임의 접근): rowkey를 통해 원하는 로우를 빠르게 가져올 수 있습니다.

3. High write throughput(높은 쓰기 처리량): LSM Tree 구조를 보면 알 수 있듯이 memtable에 데이터를 쓰다보니 디스크 접근을 필요로 하지 않습니다. 또한, 값을 업데이트할 때도 기존 값이 있는 파일을 디스크에 접근해서 읽을 필요가 없습니다. 단지 데이터를 쓸 Region만 찾으면 됩니다. 그리고 데이터를 쓸 Region도 여러 RegionServer에 분산 저장되어 있어서 빠르게 처리할 수 있습니다.


## References
> https://suewoon-ryu.medium.com/hbase-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-part-1-2fcc7a2413b1   
https://velog.io/@yunhongmin/SSTable-data-storage

